{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d678a-765d-4436-ac92-c7e4deaa2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Overview\n",
    "\n",
    "This project benchmarks **AI models for quantum computing code generation**, specifically **OpenAI’s GPT-4o** and **Anthropic’s Claude-3.5-sonnet-20240620**. The models are prompted to generate quantum computing code with beginner-friendly explanations. The project measures performance metrics such as **latency, token usage, and throughput**, and presents results in structured JSON and Markdown tables for easy comparison.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Generates **quantum computing code with explanations** for educational purposes  \n",
    "- Measures **inference performance**: latency, total tokens, tokens per second, and prompt vs completion tokens  \n",
    "- Produces **structured JSON output** for analysis or visualization  \n",
    "- Displays benchmarking results in **Markdown tables** for readability  \n",
    "\n",
    "## Tech Stack\n",
    "\n",
    "- **Models**: GPT-4o, Claude-3.5-sonnet-20240620  \n",
    "- **Language**: Python  \n",
    "- **Libraries**: OpenAI, Anthropic, time, json, IPython.display, tabulate  \n",
    "\n",
    "## Purpose\n",
    "\n",
    "Compare LLM performance across models, highlighting latency, token usage, and throughput in a structured benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e5ada-83f9-4b7c-bba6-44eba59dee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d34d9-ddb8-492c-a6f5-cea99b283fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHRPIC_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(f\"OpenAI API Key not set\")\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(f\"Anthropic API Key not set\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12fe1106-9d07-477f-91a9-d78f64222997",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "# OPENAI_MODEL = \"gpt-4.1-mini\"\n",
    "# CLAUDE_MODEL = \"claude-3-5-haiku-latest\"\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b19f07-b2e1-4eb7-9e43-75c0640a460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a Quantum Computing PHD Professor and Developer.\"\n",
    "system_message += \"You write quantum computing code and teach it to middle school students in an easy and digestible way.\"\n",
    "system_message += \"Assume your students have never heard or know anything about quantum computing.\"\n",
    "system_message += \"Every part of your code has coments and explains clearly what it is doing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a5a16-6d4f-4555-89b5-23e0c6218b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def user_prompt_for():    \n",
    "    user_prompt = \"Write quantum computing code and explain it in simple terms, using examples.\"\n",
    "    user_prompt += \"Add comments to each part of the code so it is very clear and easy to understand.\"\n",
    "    return user_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d0d1d-efd4-4c1d-b77a-b5b2c81fee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def messages_for():    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for()}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384847c3-df79-49c2-afe2-06dfd3bfc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_openai_code():\n",
    "    response = openai.chat.completions.create(\n",
    "        model = OPENAI_MODEL,\n",
    "        messages = messages_for(),\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf160823-7ed8-4d8e-84ba-e3b0db1c1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_claude_code():\n",
    "    response = claude.messages.create(\n",
    "        model = CLAUDE_MODEL,\n",
    "        max_tokens = 2000,\n",
    "        system = system_message,\n",
    "        messages = [{\"role\": \"user\", \"content\": user_prompt_for()}],\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5688a-0366-414d-a48e-2dc7a6669352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_code_markdown(model_name):\n",
    "    if OPENAI_MODEL in model_name:\n",
    "        response = generate_openai_code()\n",
    "        code = response.choices[0].message.content\n",
    "        display(Markdown(code))\n",
    "    elif CLAUDE_MODEL in model_name:\n",
    "        response = generate_claude_code()\n",
    "        code = response.content[0].text\n",
    "        display(Markdown(code))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007d40e-0206-485a-985c-565e9d5f996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_code_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13945be-b6c1-4073-811f-0697fffce6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_code_markdown(CLAUDE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9942a4c-1326-4d03-878e-d3b9ae25cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model_name):\n",
    "    start_time = time.time()\n",
    "    if OPENAI_MODEL in model_name:\n",
    "        model_name = OPENAI_MODEL\n",
    "    elif CLAUDE_MODEL in model_name:\n",
    "        model_name = CLAUDE_MODEL\n",
    "    response = generate_code()\n",
    "    end_time = time.time()\n",
    "    latency_seconds = end_time - start_time\n",
    "\n",
    "    usage = getattr(response, \"usage\", None)\n",
    "   \n",
    "    if usage:\n",
    "        total_tokens = response.usage.total_tokens\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "    else:\n",
    "        total_tokens = prompt_tokens = completion_tokens = 0\n",
    "        \n",
    "    tokens_per_second = (\n",
    "        total_tokens / latency_seconds if latency_seconds > 0 else 0\n",
    "    )    \n",
    "\n",
    "\n",
    "    \n",
    "    # Structured output\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"latency_seconds\": latency_seconds,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,   \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a55835-0507-483b-bb40-b403b0874062",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(OPENAI_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76911b-8a07-4823-a70a-6c408177108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(CLAUDE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e15ea-0470-463f-a303-251f520a6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3cd26-cd80-455d-af85-61cf4b66d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from tabulate import tabulate \n",
    "def display_benchmark_table(models):\n",
    "    results = []\n",
    "    for model_name in models:\n",
    "        result = benchmark_model(model_name)\n",
    "        results.append(result)\n",
    "    table = tabulate(results, headers=\"keys\", tablefmt=\"github\")\n",
    "    display(Markdown(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbf68e43-175a-47a5-bc0e-a7e2e0e37be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| model_name                 |   latency_seconds |   total_tokens |   tokens_per_second |   prompt_tokens |   completion_tokens |\n",
       "|----------------------------|-------------------|----------------|---------------------|-----------------|---------------------|\n",
       "| gpt-4o                     |           12.4664 |            854 |             68.5043 |             101 |                 753 |\n",
       "| claude-3-5-sonnet-20240620 |           13.7089 |            954 |             69.59   |             101 |                 853 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_benchmark_table([OPENAI_MODEL, CLAUDE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93b80d8e-23f7-4a4d-a5f5-3c32b5111eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_benchmark_json(models):\n",
    "    results = [benchmark_model(m) for m in models]\n",
    "    json_results = json.dumps(results, indent=2)\n",
    "    display(Markdown(f\"```json\\n{json_results}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "644262c9-fab4-4a62-94f7-e66e6f84f603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "[\n",
       "  {\n",
       "    \"model_name\": \"gpt-4o\",\n",
       "    \"latency_seconds\": 17.660048246383667,\n",
       "    \"total_tokens\": 863,\n",
       "    \"tokens_per_second\": 48.86736366514291,\n",
       "    \"prompt_tokens\": 101,\n",
       "    \"completion_tokens\": 762\n",
       "  },\n",
       "  {\n",
       "    \"model_name\": \"claude-3-5-sonnet-20240620\",\n",
       "    \"latency_seconds\": 14.692603349685669,\n",
       "    \"total_tokens\": 902,\n",
       "    \"tokens_per_second\": 61.3914347602188,\n",
       "    \"prompt_tokens\": 101,\n",
       "    \"completion_tokens\": 801\n",
       "  }\n",
       "]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_benchmark_json([OPENAI_MODEL, CLAUDE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce01221-d447-4bbb-901a-0de3b3ae47bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
