{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedcd287-8be6-42a5-82af-c21f2dc0ea8f",
   "metadata": {},
   "source": [
    "# Fine-Tuned vs RAG: Linux Command Knowledge Assistant (RHEL 9)\n",
    "\n",
    "## Overview\n",
    "This project compares two approaches for building a Linux terminal assistant focused on Red Hat Enterprise Linux 9 (RHCSA/RHCE) commands:\n",
    "\n",
    "1. Retrieval-Augmented Generation (RAG) using:\n",
    "   - LangChain\n",
    "   - HuggingFace embeddings (`sentence-transformers/all-MiniLM-L6-v2`)\n",
    "   - Chroma vector database\n",
    "\n",
    "2. Fine-tuning an OpenAI model (`gpt-4o-mini-2024-07-18`) on the same dataset of Linux commands and descriptions from Kaggle.\n",
    "\n",
    "Both pipelines used the same Kaggle dataset (`cyberprince/linux-terminal-commands-dataset`), containing 599 short command–description pairs.  \n",
    "No text chunking was applied, as each record was already short.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51901c0a-0198-4c67-a422-08b217dce1b6",
   "metadata": {},
   "source": [
    "## RAG Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fededb-3512-45fe-911b-9a8b67097ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from huggingface_hub import login\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9506b3-2dcf-44ee-a0a4-00ee719c22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39b88e-ee7c-42fa-9fbc-f837b3b694ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment values\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9bcfe-75c1-431d-aebc-9530e1bcd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "import time\n",
    "db_name = f\"vector_db_{int(time.time())}\"\n",
    "print(f\"Using database: {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd089ed-b0f0-40fc-95ce-7d782ebd8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbe2ae-45cd-4588-88a6-6a9ec1abf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download kaggle dataset\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(kagglehub.dataset_download(\"cyberprince/linux-terminal-commands-dataset\"))\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# find the jsonl file automatically\n",
    "json_files = list(path.glob(\"**/*.jsonl\"))\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No .jsonl file found under {path}\")\n",
    "dataset_path = json_files[0]\n",
    "print(\"Using dataset file:\", dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ef5d7-b690-4ffd-b9f7-d35894db8bbd",
   "metadata": {},
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd480c-6877-4247-a339-7a5a2e636b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b90564-dc8e-46cc-b489-be42a28e8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jsonl dataset into pandas dataframe \n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# load jsonl file line by line\n",
    "data = []\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        \n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(data)      \n",
    "\n",
    "print(df.iloc[0])\n",
    "\n",
    "#keep only the columns needed and rename them\n",
    "df = df.rename(columns={\"command\": \"instruction\", \"description\": \"output\"})\n",
    "\n",
    "df = df[[\"instruction\", \"output\"]]\n",
    "\n",
    "#convert to hf dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b1521-086e-49c5-b85d-68c3eec0d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create langchain documents for vectorization\n",
    "\n",
    "text_column = \"instruction\"\n",
    "answer_column = \"output\"\n",
    "\n",
    "def add_metadata(record):\n",
    "    metadata = {k: v for k, v in record.items() if k not in [text_column, answer_column]}\n",
    "    metadata[\"doc_type\"] = \"linux_commands\"\n",
    "    metadata[\"instruction\"] = record[text_column]\n",
    "    metadata[\"answer\"] = record[answer_column]\n",
    "    content = f\"Instruction: {record[text_column]}\\nAnswer: {record[answer_column]}\"\n",
    "    return Document(page_content=content, metadata=metadata)\n",
    "\n",
    "#convert each row into a langchain document\n",
    "documents = [add_metadata(record) for record in dataset if record.get(text_column)]\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from dataset\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeed024-e627-48d1-b6bb-9eca92a8ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents[:5]):\n",
    "    print(f\"Document {i}: {len(doc.page_content)} chars\")\n",
    "    print(f\"Content preview: {doc.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05002-99cf-4555-980c-76f60efdbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed and store vectores in chroma db\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#clear old db if exists\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name).delete_collection()\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding=embeddings, \n",
    "    persist_directory=db_name\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef4c5db-4565-4ac1-9ae5-c6383c0e1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize embeddings \n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db652839-1110-4491-ab71-bcfdb5df3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize in 2d using tsne\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "\n",
    "unique_doc_types = list(set(doc_types))\n",
    "cmap = plt.get_cmap(\"tab10\", len(unique_doc_types))\n",
    "color_map = {t: cmap(i) for i, t in enumerate(unique_doc_types)}\n",
    "colors = [color_map[t] for t in doc_types]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(\n",
    "    reduced_vectors[:, 0],\n",
    "    reduced_vectors[:,1],\n",
    "    c=colors,\n",
    "    s=20,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "plt.title(\"2D visualization of vector embeddings\")\n",
    "plt.xlabel(\"TSNE 1\")\n",
    "plt.ylabel(\"TSNE 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe452e3d-fb92-44ab-9153-51d3a84f3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build RAG chatbot\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# conversational RAG chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True #show what does are retrieved\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b42ef0-41b5-4d4e-8b0e-4d47bd9c9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Conversation chain created successfully\")\n",
    "\n",
    "# Now test it\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing RAG...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with a command we KNOW exists in the dataset\n",
    "test_question = \"What does the 'cd' command do?\"\n",
    "result = conversation_chain.invoke({\"question\": test_question})\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nSource documents retrieved: {len(result['source_documents'])}\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"  Source {i+1}: {doc.metadata.get('instruction', 'N/A')}\")\n",
    "\n",
    "# Now test with 'man' (which probably doesn't exist)\n",
    "test_question = \"What does the 'man' command do?\"\n",
    "result = conversation_chain.invoke({\"question\": test_question})\n",
    "print(f\"\\nQuestion: {test_question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nSource documents retrieved: {len(result['source_documents'])}\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"  Source {i+1}: {doc.metadata.get('instruction', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11c145-fc14-4a55-929d-ab1911fcc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f352e3-937f-46e1-928c-6b51849bd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a command that exists\n",
    "result = conversation_chain.invoke({\"question\": \"What does the 'cd' command do?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c605975-1c42-4eb6-a918-26f2d0709947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch gradio UI\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489b432-7889-4b3d-83cf-797daa225514",
   "metadata": {},
   "source": [
    "## Fine Tune Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b864da0-3eda-42eb-8183-1fe5be458d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HF_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2ab5b-d6cb-4590-9308-38e30dcbb858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b48c8e-ac03-4fbd-892f-deed14b8af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(query):\n",
    "    system_message = (\n",
    "        \"You are a Linux terminal assistant and Red Hat certification expert for RHEL 9 \"\n",
    "        \"(RHCSA and RHCE). You always give accurate, concise, and command-focused explanations \"\n",
    "        \"based on the official RHEL 9 documentation. If a Linux command is asked, explain its purpose, \"\n",
    "        \"syntax, options, and an example of real-world use.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        \"Please answer based on Red Hat Enterprise Linux 9 standards and certification expectations.\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791eb7d4-2af9-4d29-ba30-8420c2d3b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to jsonl for finetuning\n",
    "import json\n",
    "\n",
    "def make_jsonl(dataset, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in dataset:\n",
    "            messages =[\n",
    "                {\"role\": \"system\", \"content\": \"You are a Linux terminal assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "                {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "            ]\n",
    "            f.write(json.dumps({\"messages\": messages}) + \"\\n\")\n",
    "\n",
    "make_jsonl(train_dataset, \"train.jsonl\")\n",
    "make_jsonl(val_dataset, \"val.jsonl\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed9938-c41f-4cdb-a40a-5dbf7afe1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725c433-f5f0-4db1-9e1a-43cd911c6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning \n",
    "\n",
    "\n",
    "train_file = openai.files.create(file=open(\"train.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "val_file = openai.files.create(file=open(\"val.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "\n",
    "job = openai.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    validation_file=val_file.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    hyperparameters={\"n_epochs\": 2},\n",
    "    suffix=\"linux-commands\"\n",
    ")\n",
    "print(\"Fine-tuning job:\", job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908d2a1-fbc8-49a9-9459-bb19f666ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this retrieves the fine tuned job id\n",
    "job_id = job.id\n",
    "\n",
    "result = openai.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "#define separate varibale for finetuned model\n",
    "MODEL_FINETUNED = result.fine_tuned_model\n",
    "print(\"Fine-tuned model ID:\", MODEL_FINETUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe33a2-09e4-4f01-8ce3-6d9e53219723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG vs Fine-Tuned\n",
    "query = \"What does the 'man' command do?\"\n",
    "\n",
    "#RAG based response \n",
    "rag_result = conversation_chain.invoke({\"question\": query})\n",
    "rag_response = rag_result[\"answer\"]\n",
    "\n",
    "messages = messages_for(query)\n",
    "\n",
    "\n",
    "#Fine-tuned model\n",
    "fine_tuned_response = openai.chat.completions.create(\n",
    "    model=MODEL_FINETUNED,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\n Base Model RAG Response:\\n\", rag_response)\n",
    "print(\"\\n Fine-tuned Model Response:\\n\", fine_tuned_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af88859-320a-4cc5-a80f-6dded14988a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG vs Fine-Tuned\n",
    "query = \"What does the 'cd' command do?\"\n",
    "\n",
    "#RAG based response \n",
    "rag_result = conversation_chain.invoke({\"question\": query})\n",
    "rag_response = rag_result[\"answer\"]\n",
    "\n",
    "messages = messages_for(query)\n",
    "\n",
    "\n",
    "#Fine-tuned model\n",
    "fine_tuned_response = openai.chat.completions.create(\n",
    "    model=MODEL_FINETUNED,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\n Base Model RAG Response:\\n\", rag_response)\n",
    "print(\"\\n Fine-tuned Model Response:\\n\", fine_tuned_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1eefb-36e1-45dd-9e5c-608a26ff3647",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Both models used the same dataset but behaved differently because of how each method leverages data.\n",
    "\n",
    "The **RAG system** could only respond to commands explicitly stored in its vector database.  \n",
    "Because the `man` command was not in the dataset, it correctly answered *“I don’t know.”*\n",
    "\n",
    "The **fine-tuned model**, on the other hand, generalized from similar patterns and produced a correct, human-like answer for `man` even though it never saw that example during training.\n",
    "\n",
    "This demonstrates that:\n",
    "\n",
    "- **Fine-tuning** can generalize beyond the specific examples in the dataset.  \n",
    "- **RAG** strictly depends on dataset completeness and coverage.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
